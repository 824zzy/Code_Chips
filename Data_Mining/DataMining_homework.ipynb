{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DataMining-homework.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/824zzy/Code_Chips/blob/master/Data_Mining/DataMining_homework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "glGlKj5qPiR3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classification and analysis of text data\n",
        "#### Affilication: BUPT\n",
        "#### References\n",
        "- [Python爬虫之爬取动态页面数据](https://blog.csdn.net/SKI_12/article/details/78411824)\n",
        "- [6 Easy Steps to Learn Naive Bayes Algorithm (with codes in Python and R)](https://www.analyticsvidhya.com/blog/2017/09/naive-bayes-explained/)\n",
        "- [Naive Bayes Tutorial: Naive Bayes Classifier in Python](https://dzone.com/articles/naive-bayes-tutorial-naive-bayes-classifier-in-pyt)\n",
        "- [Let's implement a Gaussian Naive Bayes classifier in Python](https://www.antoniomallia.it/lets-implement-a-gaussian-naive-bayes-classifier-in-python.html)\n",
        "- [Support Vector Machines with Scikit-learn](https://www.datacamp.com/community/tutorials/svm-classification-scikit-learn-python)"
      ]
    },
    {
      "metadata": {
        "id": "yBqj04YJonl2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Crawler demo\n"
      ]
    },
    {
      "metadata": {
        "id": "k8fuXnEKqU4h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1176
        },
        "outputId": "33ed1d11-fa16-4eb6-ac41-5e11a6863253"
      },
      "cell_type": "code",
      "source": [
        "# scrapy especially for using XPATH to parse the html tree\n",
        "!pip install scrapy\n",
        "# a common tool for displaying the processing of ForLoop\n",
        "!pip install tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scrapy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/12/a6197eaf97385e96fd8ec56627749a6229a9b3178ad73866a0b1fb377379/Scrapy-1.5.1-py2.py3-none-any.whl (249kB)\n",
            "\u001b[K    100% |████████████████████████████████| 256kB 7.1MB/s \n",
            "\u001b[?25hCollecting Twisted>=13.1.0 (from scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/0e/a72d85a55761c2c3ff1cb968143a2fd5f360220779ed90e0fadf4106d4f2/Twisted-18.9.0.tar.bz2 (3.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.1MB 11.1MB/s \n",
            "\u001b[?25hCollecting parsel>=1.1 (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/96/69/d1d5dba5e4fecd41ffd71345863ed36a45975812c06ba77798fc15db6a64/parsel-1.5.1-py2.py3-none-any.whl\n",
            "Collecting pyOpenSSL (from scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/af/9d29e6bd40823061aea2e0574ccb2fcf72bfd6130ce53d32773ec375458c/pyOpenSSL-18.0.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 21.9MB/s \n",
            "\u001b[?25hCollecting w3lib>=1.17.0 (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/37/94/40c93ad0cadac0f8cb729e1668823c71532fd4a7361b141aec535acb68e3/w3lib-1.19.0-py2.py3-none-any.whl\n",
            "Collecting lxml (from scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/a4/9eea8035fc7c7670e5eab97f34ff2ef0ddd78a491bf96df5accedb0e63f5/lxml-4.2.5-cp36-cp36m-manylinux1_x86_64.whl (5.8MB)\n",
            "\u001b[K    100% |████████████████████████████████| 5.8MB 7.2MB/s \n",
            "\u001b[?25hCollecting PyDispatcher>=2.0.5 (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/cd/37/39aca520918ce1935bea9c356bcbb7ed7e52ad4e31bff9b943dfc8e7115b/PyDispatcher-2.0.5.tar.gz\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.11.0)\n",
            "Collecting service-identity (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/29/fa/995e364220979e577e7ca232440961db0bf996b6edaf586a7d1bd14d81f1/service_identity-17.0.0-py2.py3-none-any.whl\n",
            "Collecting queuelib (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/85/ae64e9145f39dd6d14f8af3fa809a270ef3729f3b90b3c0cf5aa242ab0d4/queuelib-1.5.0-py2.py3-none-any.whl\n",
            "Collecting cssselect>=0.9 (from scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/7b/44/25b7283e50585f0b4156960691d951b05d061abf4a714078393e51929b30/cssselect-1.0.3-py2.py3-none-any.whl\n",
            "Collecting zope.interface>=4.4.2 (from Twisted>=13.1.0->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/17/1d198a6aaa9aa4590862fe3d3a2ed7dd808050cab4eebe8a2f2f813c1376/zope.interface-4.6.0-cp36-cp36m-manylinux1_x86_64.whl (167kB)\n",
            "\u001b[K    100% |████████████████████████████████| 174kB 27.7MB/s \n",
            "\u001b[?25hCollecting constantly>=15.1 (from Twisted>=13.1.0->scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/b9/65/48c1909d0c0aeae6c10213340ce682db01b48ea900a7d9fce7a7910ff318/constantly-15.1.0-py2.py3-none-any.whl\n",
            "Collecting incremental>=16.10.1 (from Twisted>=13.1.0->scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/1d/c98a587dc06e107115cf4a58b49de20b19222c83d75335a192052af4c4b7/incremental-17.5.0-py2.py3-none-any.whl\n",
            "Collecting Automat>=0.3.0 (from Twisted>=13.1.0->scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/a3/86/14c16bb98a5a3542ed8fed5d74fb064a902de3bdd98d6584b34553353c45/Automat-0.7.0-py2.py3-none-any.whl\n",
            "Collecting hyperlink>=17.1.1 (from Twisted>=13.1.0->scrapy)\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/b6/84d0c863ff81e8e7de87cff3bd8fd8f1054c227ce09af1b679a8b17a9274/hyperlink-18.0.0-py2.py3-none-any.whl\n",
            "Collecting PyHamcrest>=1.9.0 (from Twisted>=13.1.0->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d5/d37fd731b7d0e91afcc84577edeccf4638b4f9b82f5ffe2f8b62e2ddc609/PyHamcrest-1.9.0-py2.py3-none-any.whl (52kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 22.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=13.1.0->scrapy) (18.2.0)\n",
            "Collecting cryptography>=2.2.1 (from pyOpenSSL->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/18/1583e40c38ff8572c42e56ce17b95357a9ebb91375cfbd7aad63cac9a32e/cryptography-2.4.1-cp34-abi3-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K    100% |████████████████████████████████| 2.1MB 13.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity->scrapy) (0.4.4)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity->scrapy) (0.2.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.4.2->Twisted>=13.1.0->scrapy) (40.6.2)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=13.1.0->scrapy) (2.6)\n",
            "Collecting cffi!=1.11.3,>=1.7 (from cryptography>=2.2.1->pyOpenSSL->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/c0/47db8f624f3e4e2f3f27be03a93379d1ba16a1450a7b1aacfa0366e2c0dd/cffi-1.11.5-cp36-cp36m-manylinux1_x86_64.whl (421kB)\n",
            "\u001b[K    100% |████████████████████████████████| 430kB 23.3MB/s \n",
            "\u001b[?25hCollecting asn1crypto>=0.21.0 (from cryptography>=2.2.1->pyOpenSSL->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/cd/35485615f45f30a510576f1a56d1e0a7ad7bd8ab5ed7cdc600ef7cd06222/asn1crypto-0.24.0-py2.py3-none-any.whl (101kB)\n",
            "\u001b[K    100% |████████████████████████████████| 102kB 30.5MB/s \n",
            "\u001b[?25hCollecting pycparser (from cffi!=1.11.3,>=1.7->cryptography>=2.2.1->pyOpenSSL->scrapy)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/68/9e/49196946aee219aead1290e00d1e7fdeab8567783e83e1b9ab5585e6206a/pycparser-2.19.tar.gz (158kB)\n",
            "\u001b[K    100% |████████████████████████████████| 163kB 29.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: Twisted, PyDispatcher, pycparser\n",
            "  Running setup.py bdist_wheel for Twisted ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/57/2e/89/11ba83bc08ac30a5e3a6005f0310c78d231b96a270def88ca0\n",
            "  Running setup.py bdist_wheel for PyDispatcher ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/88/99/96/cfef6665f9cb1522ee6757ae5955feedf2fe25f1737f91fa7f\n",
            "  Running setup.py bdist_wheel for pycparser ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/f2/9a/90/de94f8556265ddc9d9c8b271b0f63e57b26fb1d67a45564511\n",
            "Successfully built Twisted PyDispatcher pycparser\n",
            "Installing collected packages: zope.interface, constantly, incremental, Automat, hyperlink, PyHamcrest, Twisted, w3lib, lxml, cssselect, parsel, pycparser, cffi, asn1crypto, cryptography, pyOpenSSL, PyDispatcher, service-identity, queuelib, scrapy\n",
            "Successfully installed Automat-0.7.0 PyDispatcher-2.0.5 PyHamcrest-1.9.0 Twisted-18.9.0 asn1crypto-0.24.0 cffi-1.11.5 constantly-15.1.0 cryptography-2.4.1 cssselect-1.0.3 hyperlink-18.0.0 incremental-17.5.0 lxml-4.2.5 parsel-1.5.1 pyOpenSSL-18.0.0 pycparser-2.19 queuelib-1.5.0 scrapy-1.5.1 service-identity-17.0.0 w3lib-1.19.0 zope.interface-4.6.0\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.28.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t4FNo-wfptA3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import scrapy\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jOauSdlhqN4t",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Auxiliary functions for crawler"
      ]
    },
    {
      "metadata": {
        "id": "wPJY_KvLczuC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Generate Headers Randomly"
      ]
    },
    {
      "metadata": {
        "id": "HmuzDTj4cRga",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "c9f4bce6-5b93-4e72-8abc-25ed7ef1a73b"
      },
      "cell_type": "code",
      "source": [
        "import platform\n",
        "import random\n",
        "\n",
        "\n",
        "userAgent_file = [\n",
        "\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.835.163 Safari/535.1\",\n",
        "\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0) Gecko/20100101 Firefox/6.0\",\n",
        "\"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.50 (KHTML, like Gecko) Version/5.1 Safari/534.50\",\n",
        "\"Opera/9.80 (Windows NT 6.1; U; zh-cn) Presto/2.9.168 Version/11.50\",\n",
        "\"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; Win64; x64; Trident/5.0; .NET CLR 2.0.50727; SLCC2; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; .NET4.0C; Tablet PC 2.0; .NET4.0E)\",\n",
        "\"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; WOW64; Trident/4.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; .NET4.0C; InfoPath.3)\",\n",
        "\"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; GTB7.0)\",\n",
        "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)\",\n",
        "\"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1)\",\n",
        "\"Mozilla/5.0 (Windows; U; Windows NT 6.1; ) AppleWebKit/534.12 (KHTML, like Gecko) Maxthon/3.0 Safari/534.12\",\n",
        "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; .NET4.0C; .NET4.0E)\",\n",
        "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; .NET4.0C; .NET4.0E; SE 2.X MetaSr 1.0)\",\n",
        "\"Mozilla/5.0 (Windows; U; Windows NT 6.1; en-US) AppleWebKit/534.3 (KHTML, like Gecko) Chrome/6.0.472.33 Safari/534.3 SE 2.X MetaSr 1.0\",\n",
        "\"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; .NET4.0C; .NET4.0E)\",\n",
        "\"Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/13.0.782.41 Safari/535.1 QQBrowser/6.9.11079.201\",\n",
        "\"Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 6.1; WOW64; Trident/5.0; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; Media Center PC 6.0; InfoPath.3; .NET4.0C; .NET4.0E) QQBrowser/6.9.11079.201\",\n",
        "\"Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)\",\n",
        "]\n",
        "class Headers:\n",
        "    @staticmethod\n",
        "    def getHeaders():\n",
        "        userAgentList = []\n",
        "        for line in userAgent_file:\n",
        "            userAgentList.append({\n",
        "                'User-Agent': line.strip(),\n",
        "                #'User-Agent': 'Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html) ',\n",
        "                'Referer': 'http://cn.bing.com/',\n",
        "                'X-Forwarded-For': '%s.%s.%s.%s' % (\n",
        "                random.randint(50, 250), random.randint(50, 250), random.randint(50, 250), random.randint(50, 250)),\n",
        "                'CLIENT-IP': '%s.%s.%s.%s' % (\n",
        "                random.randint(50, 250), random.randint(50, 250), random.randint(50, 250), random.randint(50, 250))\n",
        "            })\n",
        "        userAgent = random.sample(userAgentList, 1)\n",
        "        return userAgent[0]\n",
        "\n",
        "# test case\n",
        "print(Headers.getHeaders())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'User-Agent': 'Mozilla/5.0 (Windows; U; Windows NT 6.1; ) AppleWebKit/534.12 (KHTML, like Gecko) Maxthon/3.0 Safari/534.12', 'Referer': 'http://cn.bing.com/', 'X-Forwarded-For': '244.250.150.165', 'CLIENT-IP': '88.80.104.111'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GBDdbPM6F3Gk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Base class for Crawlers"
      ]
    },
    {
      "metadata": {
        "id": "SApOWBpZowQ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "\n",
        "\n",
        "class Finance_base(object):\n",
        "  def __init__(self, website, website_url, header=None):\n",
        "    \"\"\"\n",
        "    \n",
        "    \"\"\"\n",
        "    self.cookies = {}\n",
        "    self.website = website\n",
        "    self.website_url = website_url\n",
        "    self.header = header\n",
        "    self.entrance_html = requests.get(self.website_url).content\n",
        "    self.sections = self.section_list()\n",
        "    \n",
        "  def section_list(self):\n",
        "    pass\n",
        "  \n",
        "  def section_urls(self):\n",
        "    pass\n",
        "  \n",
        "  def parse_body(self):\n",
        "    pass\n",
        "    \n",
        "  def ajax_url(self):\n",
        "    pass\n",
        "  \n",
        "  def article_urls_dict(self):\n",
        "    pass\n",
        "  \n",
        "  def display(self):\n",
        "    web_html = requests.get(self.website_url).content\n",
        "    sections = self.section_list()\n",
        "    print(\"sections: \", sections)\n",
        "    sec_urls = self.section_urls()\n",
        "    print(\"sections urls: \", sec_urls)\n",
        "    sec_dict = dict(zip(sections, sec_urls))\n",
        "#     atk_urls = latest_urls(sec_dict, self.website)\n",
        "    history_urls = self.ajax_urls()\n",
        "    print(\"history urls: \", history_urls)\n",
        "    \n",
        "  \n",
        "\n",
        "  \n",
        "  \n",
        "  \n",
        "# def section_list(entrance_html, website):\n",
        "#   \"\"\"get section from each website. Sections could be seem as the label of \n",
        "#      classification in the case of saving manpower.\n",
        "  \n",
        "#   # Arguments:\n",
        "#     entrance_html:\n",
        "#     website:\n",
        "  \n",
        "#   # Returns:\n",
        "#     section_urls:\n",
        "#   \"\"\"  \n",
        "#   if website == \"经济参考网\":\n",
        "#     section = scrapy.Selector(text=entrance_html).xpath('.//div[@class=\"box_mid\"]/div[@class=\"b_yw\"]//p/a/text()').extract()\n",
        "#     section_list = [item for item in section]\n",
        "#     section_list.append(\"要闻\")\n",
        "#   return section_list\n",
        "  \n",
        "  \n",
        "# def section_urls(entrance_html, website):\n",
        "#   \"\"\"get urls from each section in “经济参考网”\n",
        "  \n",
        "#   # Arguments:\n",
        "#     entrance_html:\n",
        "#     website:\n",
        "  \n",
        "#   # Returns:\n",
        "#     section_urls:\n",
        "#   \"\"\"\n",
        "#   if website == \"经济参考网\":\n",
        "#     sections = scrapy.Selector(text=entrance_html).xpath('.//div[@class=\"box_mid\"]/div[@class=\"b_yw\"]//p/a/@href').extract()\n",
        "#     section_urls = [item for item in sections]\n",
        "#     section_urls.append(\"http://jjckb.xinhuanet.com/yw.htm\")\n",
        "#   return section_urls\n",
        "\n",
        "\n",
        "# # def latest_urls(section_urls, website):\n",
        "# #   \"\"\"get articles url from each section.\n",
        "  \n",
        "  \n",
        "# #   \"\"\"\n",
        "# #   article_urls_dict = {}\n",
        "# #   if website == \"经济参考网\":\n",
        "# #     for section, section_url in section_urls.items():\n",
        "# #       section_html = requests.get(section_url).content\n",
        "# #       content_xpath = scrapy.Selector(text=section_html).xpath('.//div[@class=\"box_left\"]/ul/li/a/@href').extract()\n",
        "# #       section_url_list = [item for item in content_xpath]\n",
        "# #       article_urls_dict[section] = section_url_list\n",
        "# #   return article_urls_dict\n",
        "\n",
        "     \n",
        "  \n",
        "# def parse_body(article_urls):\n",
        "#   \"\"\"get article htmls as resources of text data and parse text \n",
        "#      data as a tuple\n",
        "      \n",
        "     \n",
        "#   \"\"\"\n",
        "#   article_htmls = [requests.get(url).content for url in article_urls]\n",
        "#   return article_htmls\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BF31LUWrc7oy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Ajax Dynamic Loading"
      ]
    },
    {
      "metadata": {
        "id": "Ciqn8isxc7Pf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_ajax_url(section, website):\n",
        "  if website == \"经济参考网\":\n",
        "    if section == \"要闻\":\n",
        "      # There are only 20 pages.\n",
        "      request_pages = [\"http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=\"+ \n",
        "                       str(page) +\"&cnt=50&attr=&tp=1&orderby=1\" for page in range(1, 21)]\n",
        "      # There are only 9 pages\n",
        "    if section == \"区域\":\n",
        "      request_pages = [\"http://qc.wa.news.cn/nodeart/list?nid=11100295&pgnum=\"+ \n",
        "                       str(page) +\"&cnt=50&attr=&tp=1&orderby=1\" for page in range(1, 10)]\n",
        "    if section == \"环球\":\n",
        "      # There are only 20 pages\n",
        "      request_pages = [\"http://qc.wa.news.cn/nodeart/list?nid=11100293&pgnum=\"+ \n",
        "                       str(page) + \"&cnt=50&attr=&tp=1&orderby=1\" for page in range(1, 21)]\n",
        "    if section == \"公司\":\n",
        "      # There are only 20 pages\n",
        "      request_pages = [\"http://qc.wa.news.cn/nodeart/list?nid=11100292&pgnum=\"+\n",
        "                       str(page)+ \"&cnt=50&attr=&tp=1&orderby=1\" for page in range(1, 21)]\n",
        "  return request_pages\n",
        "\n",
        "\n",
        "def ajax_urls(sections, website, headers):\n",
        "  \"\"\" deal with ajax dynamic loading problem.\n",
        "  \n",
        "  # Arguments:\n",
        "    sections: a list contains all the sections in website\n",
        "    website: a str represents target website\n",
        "    headers: a str we generate in previous stage\n",
        "     \n",
        "  # Returns:\n",
        "    article_urls_dict: a dict whose keys are sections and values \n",
        "      are articles' urls for this section\n",
        "    \n",
        "  \n",
        "  \"\"\"\n",
        "  article_urls_dict = {}\n",
        "  for sec in sections:\n",
        "    request_pages = generate_ajax_request(sec, website)\n",
        "    sec_urls = []\n",
        "    for page in request_pages:\n",
        "      json_str = requests.get(url=page,headers=headers).text[1:-1]\n",
        "      parsed_json = json.loads(json_str)\n",
        "      page_urls = [item[\"LinkUrl\"] for item in parsed_json[\"data\"][\"list\"]]\n",
        "      sec_urls += page_urls\n",
        "    article_urls_dict[sec] = sec_urls\n",
        "\n",
        "  return article_urls_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FUu0D2orseMx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### save text data as CSV format"
      ]
    },
    {
      "metadata": {
        "id": "E7_tBLtBsjHR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "\n",
        "class Pd_csv(object):\n",
        "  def __init__(self, name):\n",
        "    self.name = name\n",
        "    \n",
        "  def read_csv(self):\n",
        "    \"\"\" reading CSV Files with Pandas\n",
        "\n",
        "    # Arguments:\n",
        "      name: file name of website without suffix, str\n",
        "\n",
        "    # Returns:\n",
        "      df: data frame contains \n",
        "    \"\"\"\n",
        "  #   with open(name+\".csv\") as csv_file:\n",
        "  #     csv_reader = csv.reader(csv_file, delimiter)\n",
        "    df = pandas.read_csv(name+'.csv')\n",
        "  # ???\n",
        "    return df\n",
        "\n",
        "  def write_csv(self):\n",
        "    \"\"\" writing CSV Files with Pandas\n",
        "\n",
        "    Arguments:\n",
        "      name: file name of website text data without suffix, str\n",
        "\n",
        "    Returns:\n",
        "\n",
        "\n",
        "    \"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z0pq59gugW7g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Economy Crawler"
      ]
    },
    {
      "metadata": {
        "id": "e4SNPfZ3Ox9o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# class Finance_base(object):\n",
        "#   def __init__(self, website, website_url, header):\n",
        "#     \"\"\"\n",
        "    \n",
        "#     \"\"\"\n",
        "#     self.cookies = {}\n",
        "#     self.website = website\n",
        "#     self.website_url = website_url\n",
        "#     self.header = header\n",
        "    \n",
        "#   def origin_dict(self):\n",
        "#     web_html = requests.get(self.website_url).content\n",
        "#     sections = section_list(web_html, self.website)\n",
        "#     print(sections)\n",
        "#     sec_urls = section_urls(web_html, self.website)\n",
        "#     print(sec_urls)\n",
        "#     sec_dict = dict(zip(sections, sec_urls))\n",
        "# #     atk_urls = latest_urls(sec_dict, self.website)\n",
        "# #     print(atk_urls)\n",
        "#     history_urls = ajax_urls(sections, self.website, header)\n",
        "#     print(history_urls)\n",
        "    \n",
        "# #     return sections\n",
        "  \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a50PUAebOe-k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### 经济参考网\n"
      ]
    },
    {
      "metadata": {
        "id": "bV1bQKsCOdxW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "56d2b45f-f15a-4fbd-860a-fd99ca411065"
      },
      "cell_type": "code",
      "source": [
        "class Economic_Information_Daily(Finance_base):\n",
        "  def __init__(self, **kwargs):\n",
        "    super(Economic_Information_Daily, self).__init__(**kwargs)\n",
        "\n",
        "  def section_list(self):\n",
        "    \"\"\"get section from each website. Sections could be seem as the label of \n",
        "       classification in the case of saving manpower.\n",
        "\n",
        "    # Arguments:\n",
        "      entrance_html:\n",
        "      website:\n",
        "\n",
        "    # Returns:\n",
        "      section_urls:\n",
        "    \"\"\"\n",
        "    section = scrapy.Selector(text=self.website_url).xpath('.//div[@class=\"box_mid\"]/div[@class=\"b_yw\"]//p/a/text()').extract()\n",
        "    section_list = [item for item in section]\n",
        "    section_list.append(\"要闻\")\n",
        "    return section_list\n",
        "  \n",
        "  def section_urls(self):\n",
        "    \"\"\"get urls from each section in “经济参考网”\n",
        "\n",
        "    # Arguments:\n",
        "      entrance_html:\n",
        "      website:\n",
        "\n",
        "    # Returns:\n",
        "      section_urls:\n",
        "    \"\"\"\n",
        "    sections = scrapy.Selector(text=self.website_url).xpath('.//div[@class=\"box_mid\"]/div[@class=\"b_yw\"]//p/a/@href').extract()\n",
        "    section_urls = [item for item in sections]\n",
        "    section_urls.append(\"http://jjckb.xinhuanet.com/yw.htm\")\n",
        "    return section_urls\n",
        "  \n",
        "  def parse_body(self):\n",
        "    \"\"\"get article htmls as resources of text data and parse text \n",
        "       data as a tuple\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    article_htmls = [requests.get(url).content for url in article_urls]\n",
        "    return article_htmls\n",
        "  \n",
        "  def ajax_urls(self):\n",
        "    request_pages_list = []\n",
        "    for section in self.sections:\n",
        "      if section == \"要闻\":\n",
        "        # There are only 20 pages.\n",
        "        request_pages = [\"http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=\"+ \n",
        "                         str(page) +\"&cnt=50&attr=&tp=1&orderby=1\" for page in range(1, 21)]\n",
        "        request_pages_list.append(dict[]request_pages)\n",
        "        # There are only 9 pages\n",
        "      if section == \"区域\":\n",
        "        request_pages = [\"http://qc.wa.news.cn/nodeart/list?nid=11100295&pgnum=\"+ \n",
        "                         str(page) +\"&cnt=50&attr=&tp=1&orderby=1\" for page in range(1, 10)]\n",
        "        request_pages_list.append(request_pages)\n",
        "      if section == \"环球\":\n",
        "        # There are only 20 pages\n",
        "        request_pages = [\"http://qc.wa.news.cn/nodeart/list?nid=11100293&pgnum=\"+ \n",
        "                         str(page) + \"&cnt=50&attr=&tp=1&orderby=1\" for page in range(1, 21)]\n",
        "        request_pages_list.append(request_pages)\n",
        "      if section == \"公司\":\n",
        "        # There are only 20 pages\n",
        "        request_pages = [\"http://qc.wa.news.cn/nodeart/list?nid=11100292&pgnum=\"+\n",
        "                         str(page)+ \"&cnt=50&attr=&tp=1&orderby=1\" for page in range(1, 21)]\n",
        "        request_pages_list.append(request_pages)\n",
        "    return request_pages_list\n",
        "  \n",
        "  def article_urls_dict(self):\n",
        "    \"\"\" deal with ajax dynamic loading problem.\n",
        "\n",
        "    # Arguments:\n",
        "      sections: a list contains all the sections in website\n",
        "      website: a str represents target website\n",
        "      headers: a str we generate in previous stage\n",
        "\n",
        "    # Returns:\n",
        "      article_urls_dict: a dict whose keys are sections and values \n",
        "        are articles' urls for this section\n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    article_urls_dict = {}\n",
        "    for sec in sections:\n",
        "      request_pages = generate_ajax_request(sec, website)\n",
        "      sec_urls = []\n",
        "      for page in request_pages:\n",
        "        json_str = requests.get(url=page,headers=headers).text[1:-1]\n",
        "        parsed_json = json.loads(json_str)\n",
        "        page_urls = [item[\"LinkUrl\"] for item in parsed_json[\"data\"][\"list\"]]\n",
        "        sec_urls += page_urls\n",
        "      article_urls_dict[sec] = sec_urls\n",
        "\n",
        "    return article_urls_dict\n",
        "\n",
        "header = Headers.getHeaders()\n",
        "spider = Economic_Information_Daily(website=\"经济参考网\", website_url=\"http://www.jjckb.cn/\", header=header)\n",
        "print('\\n'*2 + '='*50 + '\\n'*2)\n",
        "spider.display()\n",
        "print('\\n'*2 + '='*50 + '\\n'*2)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n",
            "sections:  ['要闻']\n",
            "sections urls:  ['http://jjckb.xinhuanet.com/yw.htm']\n",
            "history urls:  [['http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=1&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=2&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=3&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=4&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=5&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=6&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=7&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=8&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=9&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=10&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=11&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=12&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=13&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=14&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=15&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=16&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=17&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=18&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=19&cnt=50&attr=&tp=1&orderby=1', 'http://qc.wa.news.cn/nodeart/list?nid=11100290&pgnum=20&cnt=50&attr=&tp=1&orderby=1']]\n",
            "\n",
            "\n",
            "==================================================\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "i3zJcwjGVG57",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Preprocessing\n"
      ]
    },
    {
      "metadata": {
        "id": "TKjUkH_hZUoa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import re\n",
        "def abstract_html(news_html, website):\n",
        "  \"\"\" Get section, title, content from news html.\n",
        "  \n",
        "  Arguments:\n",
        "    news_html:\n",
        "    website\n",
        "  \n",
        "  Return:\n",
        "    title:\n",
        "    text:\n",
        "    \n",
        "  \"\"\"\n",
        "    for html in news_html:\n",
        "      title = selector.xpath('//div[@class=\"xl_left\"]/div[@class=\"top_tit\"]/text()').extract()[0]\n",
        "    div = selector.xpath('//div[@class=\"mainCon\"]')\n",
        "    Final_text = ''\n",
        "    for p in div.xpath('.//p/text()'):\n",
        "        text = text + p.extract().strip()\n",
        "    return title, text\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OS6zL9rBdreX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Naive Bayes implementation\n",
        "In this section, we implement Naive Bayes as Baseline."
      ]
    },
    {
      "metadata": {
        "id": "92u1fCDTd0Wl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Import Libraries and Loading Datasets\n",
        "we are going to use the IRIS dataset, which comes with the Sckit-learn library. The dataset contains 3 classes of 50 instances.\n"
      ]
    },
    {
      "metadata": {
        "id": "cY7uv4krzX9Z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn import svm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FsMD_ZgHOFwL",
        "colab_type": "code",
        "outputId": "0be6653e-ebd9-4ecd-d678-a3ae6b639a7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2686
        }
      },
      "cell_type": "code",
      "source": [
        "# just use these data for sake of testing algorithm\n",
        "dataset = datasets.load_iris()\n",
        "print('data is :\\n', dataset.data)\n",
        "print('data label is:\\n', dataset.target)\n",
        "# 50% training and 50% test\n",
        "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.2,random_state=109) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data is :\n",
            " [[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]]\n",
            "data label is:\n",
            " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uWq86Pyaigoy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build Naive Bayes model as baseline"
      ]
    },
    {
      "metadata": {
        "id": "558od5WwiN5O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "bayes_model = GaussianNB()\n",
        "bayes_model.fit(X_train, y_train)\n",
        "\n",
        "expected = y_test\n",
        "bayes_predicted = bayes_model.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ljt83Div0Uob",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build Support Vector Machine  Model using different kernel\n",
        "- linear: $\\langle x, x' \\rangle$.\n",
        "- polynomial: $(\\gamma\\langle x, x'\\rangle + r)^d$.  \n",
        "- rbf: $\\exp(-\\gamma \\|x-x'\\|^2)$. \n",
        "- sigmoid ($\\tanh(\\gamma \\langle x,x'\\rangle + r)$)"
      ]
    },
    {
      "metadata": {
        "id": "hXQNOdZc0T5W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Create a svm Classifier\n",
        "# Linear Kernel\n",
        "linear_svm = svm.SVC(kernel='linear') \n",
        "linear_svm.fit(X_train, y_train)\n",
        "# Polynomial Kernel\n",
        "polynomial_svm = svm.SVC(kernel='poly') \n",
        "polynomial_svm.fit(X_train, y_train)\n",
        "# rbf Kernel\n",
        "rbf_svm = svm.SVC(kernel='rbf') \n",
        "rbf_svm.fit(X_train, y_train)\n",
        "# sigmoid Kernel\n",
        "sigmoid_svm = svm.SVC(kernel='sigmoid') \n",
        "sigmoid_svm.fit(X_train, y_train)\n",
        "\n",
        "expected = y_test\n",
        "linear_svm_predicted = linear_svm.predict(X_test)\n",
        "polynomial_svm_predicted = polynomial_svm.predict(X_test)\n",
        "rbf_svm_predicted = rbf_svm.predict(X_test)\n",
        "sigmoid_svm_predicted = sigmoid_svm.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AC4ygVAmi4VJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Get Accuraccy and Recall and F1-score"
      ]
    },
    {
      "metadata": {
        "id": "glWz5jGQipHl",
        "colab_type": "code",
        "outputId": "9d325df8-f637-413c-872c-fd722d297bc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 836
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"precision recell f1-score and support of Naive Bayes: \\n\", metrics.classification_report(expected, bayes_predicted))\n",
        "print(\"precision recell f1-score and support of linear_SVM: \\n\", metrics.classification_report(expected, linear_svm_predicted))\n",
        "print(\"precision recell f1-score and support of polynomial_SVM: \\n\", metrics.classification_report(expected, polynomial_svm_predicted))\n",
        "print(\"precision recell f1-score and support of rbf_SVM: \\n\", metrics.classification_report(expected, rbf_svm_predicted))\n",
        "print(\"precision recell f1-score and support of sigmoid_SVM: \\n\", metrics.classification_report(expected, sigmoid_svm_predicted))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "precision recell f1-score and support of Naive Bayes: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          0       1.00      1.00      1.00        10\n",
            "          1       0.90      0.90      0.90        10\n",
            "          2       0.90      0.90      0.90        10\n",
            "\n",
            "avg / total       0.93      0.93      0.93        30\n",
            "\n",
            "precision recell f1-score and support of linear_SVM: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          0       1.00      1.00      1.00        10\n",
            "          1       1.00      0.80      0.89        10\n",
            "          2       0.83      1.00      0.91        10\n",
            "\n",
            "avg / total       0.94      0.93      0.93        30\n",
            "\n",
            "precision recell f1-score and support of polynomial_SVM: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          0       1.00      1.00      1.00        10\n",
            "          1       1.00      0.80      0.89        10\n",
            "          2       0.83      1.00      0.91        10\n",
            "\n",
            "avg / total       0.94      0.93      0.93        30\n",
            "\n",
            "precision recell f1-score and support of rbf_SVM: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          0       1.00      1.00      1.00        10\n",
            "          1       0.90      0.90      0.90        10\n",
            "          2       0.90      0.90      0.90        10\n",
            "\n",
            "avg / total       0.93      0.93      0.93        30\n",
            "\n",
            "precision recell f1-score and support of sigmoid_SVM: \n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "          0       0.00      0.00      0.00        10\n",
            "          1       0.00      0.00      0.00        10\n",
            "          2       0.33      1.00      0.50        10\n",
            "\n",
            "avg / total       0.11      0.33      0.17        30\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
            "  'precision', 'predicted', average, warn_for)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "PUk9QXnki9Zy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}